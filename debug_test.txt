üîß Compression worker started
üîÑ Background compression worker started
Welcome to Episodic! Type '/help' for commands or start chatting.
üîç DEBUG: No active topic found for current head node
Warning: Input is not a terminal (fd=0).
> 






   > /init --erase
üóëÔ∏è  Erasing existing database...
‚úÖ Database erased and reinitialized
[Benchmark] Database initialization: 0.00s
> /set debug true                 > /set debug true
Debug mode set to True
> /set main.max_tokens 30                         > /set main.max_tokens 30
Set main.max_tokens to 30
>   > 
> Tell me about Mars.                     > Tell me about Mars.

üîç DEBUG: Topic detection check
   Recent nodes count: 1
   Current topic: None
   Min messages before topic change: 4
   ‚ö†Ô∏è  Not enough history for topic detection
[LLM API] Thread 8430968576: Call #1
[LLM API] Model: openai/gpt-3.5-turbo (stream=True)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:321 in query_with_context()
[LLM API] Messages: 2 messages

ü§ñ DEBUG: Streaming modes - char: True, natural: False, constant: False
Mars is the fourth planet from the Sun in our solar system. Here are some 
key points about Mars:

1. Physical Characteristics: Mars is

Tokens: 252 | Cost: $0.000399 USD | Context: 1% full
üîç DEBUG: No current topic set, checking if we need to create first topic...
   DEBUG: No topics exist, checking if we should create first topic...
   DEBUG: user_node_id = f46c0e1f-c68d-4dd7-aded-469015bb50fe
   DEBUG: user_node content = 'Tell me about Mars....'
   DEBUG: should_create_first_topic: 1 user messages, threshold: 3, returning: False
üîç DEBUG: Skipping initial topic creation - not enough messages yet

[Benchmark] Message Processing: 1.29s
  - Database: 0.00s (4 calls)
  - LLM Call: 1.29s (1 calls)
> What year did Curiosity land?                               > What year did Curiosity land?

üîç DEBUG: Topic detection check
   Recent nodes count: 3
   Current topic: None
   Min messages before topic change: 4

üîç DEBUG: No current topic, only 1 total user messages (min: 4)
   Topic change detected: False
[LLM API] Thread 8430968576: Call #2
[LLM API] Model: openai/gpt-3.5-turbo (stream=True)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:321 in query_with_context()
[LLM API] Messages: 4 messages

ü§ñ DEBUG: Streaming modes - char: True, natural: False, constant: False
Curiosity, the Mars rover, landed on Mars on August 6, 2012.

Tokens: 278 | Cost: $0.000439 USD | Context: 1% full
üîç DEBUG: No current topic set, checking if we need to create first topic...
   DEBUG: No topics exist, checking if we should create first topic...
   DEBUG: user_node_id = 120bc5fc-b63f-495f-bcb1-8926d15617e6
   DEBUG: user_node content = 'What year did Curiosity land?...'
   DEBUG: should_create_first_topic: 2 user messages, threshold: 3, returning: False
üîç DEBUG: Skipping initial topic creation - not enough messages yet

[Benchmark] Message Processing: 0.58s
  - Database: 0.00s (4 calls)
  - LLM Call: 0.57s (1 calls)
> How big is Mars?                  > How big is Mars?

üîç DEBUG: Topic detection check
   Recent nodes count: 5
   Current topic: None
   Min messages before topic change: 4

üîç DEBUG: No current topic, only 2 total user messages (min: 4)
   Topic change detected: False
[LLM API] Thread 8430968576: Call #3
[LLM API] Model: openai/gpt-3.5-turbo (stream=True)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:321 in query_with_context()
[LLM API] Messages: 6 messages

ü§ñ DEBUG: Streaming modes - char: True, natural: False, constant: False
Mars has a diameter of about 6,779 kilometers (4,212 miles). This makes it 
roughly half the size of Earth.

Tokens: 311 | Cost: $0.000499 USD | Context: 1% full
üîç DEBUG: No current topic set, checking if we need to create first topic...
   DEBUG: No topics exist, checking if we should create first topic...
   DEBUG: user_node_id = 76cf2af6-a893-475c-8707-7fab740e9259
   DEBUG: user_node content = 'How big is Mars?...'
   DEBUG: should_create_first_topic: 3 user messages, threshold: 3, returning: True
   Building segment from 7 nodes (max_length=2000)
   Skipping node with empty content (role=system)
   Final segment length: 424 chars

üîç DEBUG: Topic extraction prompt:
   Model: ollama/llama3
   Prompt preview: Identify the main topic of this conversation. Reply with ONLY the topic name (1-3 words, lowercase, use hyphens for spaces).

Examples:
- Conversation about movies and directors ‚Üí movies
- Discussion of quantum physics concepts ‚Üí quantum-physics
- Debugging code and performance ‚Üí programming
- Talki...
[LLM API] Thread 8430968576: Call #4
[LLM API] Model: ollama/llama3 (stream=False)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:267 in query_llm()
[LLM API] Messages: 2 messages
[LLM API] Cost calculated: $0.000000
[LLM API] Thread 8430968576: Call #4 completed in 0.47s
[LLM API] Response length: 4 chars
---
   DEBUG: Raw topic extraction response: 'mars'
   DEBUG: Final topic name: 'mars'
üîÑ DEBUG: Current topic set to 'mars'

üìå Created initial topic: mars (from 02 to 07)

[Benchmark] Message Processing: 1.17s
  - Database: 0.00s (4 calls)
  - LLM Call: 1.16s (2 calls)
> What's the atmosphere like?                             > What's the atmosphere like?

üîç DEBUG: Topic detection check
   Recent nodes count: 7
   Current topic: ('mars', 'f46c0e1f-c68d-4dd7-aded-469015bb50fe')
   Min messages before topic change: 4
   Context preview: - User: How big is Mars?
- User: What year did Curiosity land?
- User: Tell me about Mars....
   Prompt length: 1291 chars
   Full prompt:
You are a topic-shift detection assistant.
You will receive the last n user messages and the current user message.
Your task: deduce whether the current message starts a new topic.

Preceding context:
- User: How big is Mars?
- User: What year did Curiosity land?
- User: Tell me about Mars.

New message:
What's the atmosphere like?

Step 1. Classify intent as exactly one of:
- JUST_COMMENT: Brief acknowledgment, reaction, or filler that doesn't advance conversation
- DEVELOP_TOPIC: Continuing or expanding on the current topic
- INTRODUCE_TOPIC: Starting a conversation or first substantial message
- CHANGE_TOPIC: Shifting to a completely different subject

Step 2. Determine if this is a topic shift:
- INTRODUCE_TOPIC ‚Üí YES (starting fresh)
- CHANGE_TOPIC ‚Üí YES (new subject)
- DEVELOP_TOPIC ‚Üí NO (same topic)
- JUST_COMMENT ‚Üí NO (not substantial)

Examples:
- "Tell me more" ‚Üí DEVELOP_TOPIC ‚Üí NO
- "What about its moons?" (after Mars discussion) ‚Üí DEVELOP_TOPIC ‚Üí NO
- "How do I cook pasta?" (after Mars discussion) ‚Üí CHANGE_TOPIC ‚Üí YES
- "Thanks!" ‚Üí JUST_COMMENT ‚Üí NO
- "What is Python?" (first message) ‚Üí INTRODUCE_TOPIC ‚Üí YES

Respond with ONLY a JSON object in this exact format:
{
  "intent": "<JUST_COMMENT|DEVELOP_TOPIC|INTRODUCE_TOPIC|CHANGE_TOPIC>",
  "shift": "<YES|NO>"
}
   ---End prompt---

üîç DEBUG: Topic change detection
   Model: ollama/llama3
   Recent messages: 7
   New message preview: What's the atmosphere like?...
   Topic params being used: {'temperature': 0.0, 'max_tokens': 50, 'top_p': 0.9, 'stop': []}
[LLM API] Thread 8430968576: Call #5
[LLM API] Model: ollama/llama3 (stream=False)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:267 in query_llm()
[LLM API] Messages: 2 messages
[LLM API] Cost calculated: $0.000000
[LLM API] Thread 8430968576: Call #5 completed in 1.02s
[LLM API] Response length: 48 chars
---
   LLM response: {
  "intent": "DEVELOP_TOPIC",
  "shift": "NO"
}
   Response type: <class 'str'>
   Response length: 48
   Parsed intent-based JSON: intent=DEVELOP_TOPIC, shift=NO
   ‚û°Ô∏è Continuing same topic (intent: DEVELOP_TOPIC)
   Topic change detected: False
[LLM API] Thread 8430968576: Call #6
[LLM API] Model: openai/gpt-3.5-turbo (stream=True)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:321 in query_with_context()
[LLM API] Messages: 6 messages

ü§ñ DEBUG: Streaming modes - char: True, natural: False, constant: False
Mars has a very thin atmosphere composed mainly of carbon dioxide (95.3%), 
with nitrogen (2.7%), argon (1.6

Tokens: 312 | Cost: $0.000499 USD | Context: 1% full
üîç DEBUG: Extended topic 'mars' to include new response

[Benchmark] Message Processing: 1.51s
  - Database: 0.00s (4 calls)
  - LLM Call: 1.50s (2 calls)
>   > 
> I want to learn cooking.                          > I want to learn cooking.

üîç DEBUG: Topic detection check
   Recent nodes count: 9
   Current topic: ('mars', 'f46c0e1f-c68d-4dd7-aded-469015bb50fe')
   Min messages before topic change: 4
   Context preview: - User: What's the atmosphere like?
- User: How big is Mars?
- User: What year did Curiosity land?
- User: Tell me about Mars....
   Prompt length: 1324 chars
   Full prompt:
You are a topic-shift detection assistant.
You will receive the last n user messages and the current user message.
Your task: deduce whether the current message starts a new topic.

Preceding context:
- User: What's the atmosphere like?
- User: How big is Mars?
- User: What year did Curiosity land?
- User: Tell me about Mars.

New message:
I want to learn cooking.

Step 1. Classify intent as exactly one of:
- JUST_COMMENT: Brief acknowledgment, reaction, or filler that doesn't advance conversation
- DEVELOP_TOPIC: Continuing or expanding on the current topic
- INTRODUCE_TOPIC: Starting a conversation or first substantial message
- CHANGE_TOPIC: Shifting to a completely different subject

Step 2. Determine if this is a topic shift:
- INTRODUCE_TOPIC ‚Üí YES (starting fresh)
- CHANGE_TOPIC ‚Üí YES (new subject)
- DEVELOP_TOPIC ‚Üí NO (same topic)
- JUST_COMMENT ‚Üí NO (not substantial)

Examples:
- "Tell me more" ‚Üí DEVELOP_TOPIC ‚Üí NO
- "What about its moons?" (after Mars discussion) ‚Üí DEVELOP_TOPIC ‚Üí NO
- "How do I cook pasta?" (after Mars discussion) ‚Üí CHANGE_TOPIC ‚Üí YES
- "Thanks!" ‚Üí JUST_COMMENT ‚Üí NO
- "What is Python?" (first message) ‚Üí INTRODUCE_TOPIC ‚Üí YES

Respond with ONLY a JSON object in this exact format:
{
  "intent": "<JUST_COMMENT|DEVELOP_TOPIC|INTRODUCE_TOPIC|CHANGE_TOPIC>",
  "shift": "<YES|NO>"
}
   ---End prompt---

üîç DEBUG: Topic change detection
   Model: ollama/llama3
   Recent messages: 9
   New message preview: I want to learn cooking....
   Topic params being used: {'temperature': 0.0, 'max_tokens': 50, 'top_p': 0.9, 'stop': []}
[LLM API] Thread 8430968576: Call #7
[LLM API] Model: ollama/llama3 (stream=False)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:267 in query_llm()
[LLM API] Messages: 2 messages
[LLM API] Cost calculated: $0.000000
[LLM API] Thread 8430968576: Call #7 completed in 0.97s
[LLM API] Response length: 51 chars
---
   LLM response: {
  "intent": "INTRODUCE_TOPIC",
  "shift": "YES"
}
   Response type: <class 'str'>
   Response length: 51
   Parsed intent-based JSON: intent=INTRODUCE_TOPIC, shift=YES
   ‚úÖ Topic change detected (intent: INTRODUCE_TOPIC)
   Topic change detected: True
   New topic: None
[LLM API] Thread 8430968576: Call #8
[LLM API] Model: openai/gpt-3.5-turbo (stream=True)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:321 in query_with_context()
[LLM API] Messages: 6 messages

üîç DEBUG: Topic change detected
   New topic: None
   Detection cost: $0.000000

ü§ñ DEBUG: Streaming modes - char: True, natural: False, constant: False
That's great! Cooking can be a fun and rewarding skill to develop. To get 
started, you may want to consider the following steps:

1.

Tokens: 322 | Cost: $0.000514 USD | Context: 1% full
   Building segment from 8 nodes (max_length=2000)
   Final segment length: 577 chars

üîç DEBUG: Extracting name for previous topic 'mars'
   Topic has 8 nodes
   Start node: f46c0e1f...
   End node: 480d9bf5...
   First node ID: f46c0e1f...
   Last node ID: 480d9bf5...
   Segment preview: user: Tell me about Mars.
assistant: Mars is the fourth planet from the Sun in our solar system. Here are some key points about Mars:

1. **Physical Characteristics**: Mars is
user: What year did Curi...

üîç DEBUG: Topic extraction prompt:
   Model: ollama/llama3
   Prompt preview: Identify the main topic of this conversation. Reply with ONLY the topic name (1-3 words, lowercase, use hyphens for spaces).

Examples:
- Conversation about movies and directors ‚Üí movies
- Discussion of quantum physics concepts ‚Üí quantum-physics
- Debugging code and performance ‚Üí programming
- Talki...
[LLM API] Thread 8430968576: Call #9
[LLM API] Model: ollama/llama3 (stream=False)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:267 in query_llm()
[LLM API] Messages: 2 messages
[LLM API] Cost calculated: $0.000000
[LLM API] Thread 8430968576: Call #9 completed in 0.40s
[LLM API] Response length: 4 chars
---
   DEBUG: Raw topic extraction response: 'mars'
   DEBUG: Final topic name: 'mars'
   Extracted topic name: mars
üì• Queued compression job for topic 'mars'
üîß Processing compression job for topic 'mars'
   üì¶ Queued topic 'mars' for compression
üîß Compressing topic 'mars' from f46c0e1f-c68d-4dd7-aded-469015bb50fe to 480d9bf5-acdc-474c-a5c8-1872aad75436
üîÑ DEBUG: Current topic changed from 'mars' to 'ongoing-1751088555'

üîÑ Topic changed

[Benchmark] Message Processing: 1.89s
  - Database: 0.00s (4 calls)
  - LLM Call: 1.87s (3 calls)
üîß Found 8 nodes in topic segment
> What are mother sauces?                         [LLM API] Thread 6179876864: Call #10
[LLM API] Model: ollama/llama3 (stream=False)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:267 in query_llm()
[LLM API] Messages: 2 messages
> What are mother sauces?

üîç DEBUG: Topic detection check
   Recent nodes count: 10
   Current topic: ('ongoing-1751088555', 'b9502f5b-c50d-4d6e-b52a-4934c5d474cf')
   Min messages before topic change: 4

üîç DEBUG: Skipping topic detection - current topic has only 2 user messages (min: 4, total topics: 2)
   Topic change detected: False
[LLM API] Thread 8430968576: Call #10
[LLM API] Model: openai/gpt-3.5-turbo (stream=True)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:321 in query_with_context()
[LLM API] Messages: 6 messages

ü§ñ DEBUG: Streaming modes - char: True, natural: False, constant: False
Mother sauces are the base sauces from which many other sauces can be made. 
There are five mother sauces in classical French cuisine, which are:

1.

Tokens: 324 | Cost: $0.000519 USD | Context: 1% full
üîç DEBUG: Extended topic 'ongoing-1751088555' to include new response

[Benchmark] Message Processing: 0.87s
  - Database: 0.01s (4 calls)
  - LLM Call: 0.86s (1 calls)
[LLM API] Cost calculated: $0.000000
[LLM API] Thread 6179876864: Call #11 completed in 2.31s
[LLM API] Response length: 518 chars
---

‚úÖ Auto-compressed topic 'mars' (5.7% reduction)
>   > 
> /exit       > /exit

üîç DEBUG: Finalizing topic 'ongoing-1751088555'
   Building segment from 4 nodes (max_length=2000)
   Final segment length: 364 chars

üîç DEBUG: Topic extraction prompt:
   Model: ollama/llama3
   Prompt preview: Identify the main topic of this conversation. Reply with ONLY the topic name (1-3 words, lowercase, use hyphens for spaces).

Examples:
- Conversation about movies and directors ‚Üí movies
- Discussion of quantum physics concepts ‚Üí quantum-physics
- Debugging code and performance ‚Üí programming
- Talki...
[LLM API] Thread 8430968576: Call #12
[LLM API] Model: ollama/llama3 (stream=False)
[LLM API] Called from: /Users/mhcoen/proj/episodic/episodic/llm.py:267 in query_llm()
[LLM API] Messages: 2 messages
[LLM API] Cost calculated: $0.000000
[LLM API] Thread 8430968576: Call #12 completed in 0.29s
[LLM API] Response length: 7 chars
---
   DEBUG: Raw topic extraction response: 'cooking'
   DEBUG: Final topic name: 'cooking'
   ‚úÖ Finalized topic: 'ongoing-1751088555' ‚Üí 'cooking' (1 rows)
üîÑ DEBUG: Current topic changed from 'ongoing-1751088555' to 'cooking'
Goodbye! üëã
