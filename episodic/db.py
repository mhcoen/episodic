import sqlite3
import uuid
import os
import threading
import contextlib
import logging
from typing import Optional
from .configuration import (
    DATABASE_FILENAME, MAX_DATABASE_RETRIES, FALLBACK_ID_LENGTH,
    MIN_SHORT_ID_LENGTH, SHORT_ID_MAX_LENGTH, ID_CHARSET
)

# Set up logging
logger = logging.getLogger(__name__)

# Default database path
DEFAULT_DB_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", DATABASE_FILENAME))
# Alias for backward compatibility with test scripts
DB_PATH = DEFAULT_DB_PATH

# Thread-local storage for database connections
_local = threading.local()

def get_db_path():
    """Get the database path from the environment variable or use the default."""
    return os.environ.get("EPISODIC_DB_PATH", DEFAULT_DB_PATH)

@contextlib.contextmanager
def get_connection():
    """
    Get a connection to the database.

    This function returns a context manager that ensures the connection
    is properly closed when the context exits.

    Returns:
        A SQLite database connection.
    """
    # Always create a new connection to avoid issues with thread-local storage
    # in multi-threaded environments like WebSocket tests
    connection = sqlite3.connect(get_db_path())

    try:
        # Yield the connection to the caller
        yield connection
    except Exception as e:
        # If an exception occurs, rollback and close the connection, then re-raise
        connection.rollback()
        connection.close()
        raise
    finally:
        # Commit any pending changes and close the connection
        try:
            connection.commit()
        except Exception:
            # If commit fails, try to rollback
            try:
                connection.rollback()
            except Exception:
                pass
        finally:
            connection.close()

def close_connection():
    """
    Close the database connection for the current thread.

    This function is maintained for backward compatibility but is no longer needed
    since connections are now closed automatically when the context manager exits.
    """
    # This function is now a no-op since connections are closed automatically
    pass

def database_exists():
    """Check if the database file exists and has tables."""
    db_path = get_db_path()
    if not os.path.exists(db_path):
        return False

    try:
        with get_connection() as conn:
            c = conn.cursor()
            c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='nodes'")
            return c.fetchone() is not None
    except sqlite3.Error:
        return False

def base36_encode(number):
    """
    Convert a positive integer to a base-36 string.
    """
    if not isinstance(number, int) or number < 0:
        raise ValueError("Number must be a positive integer")

    chars = '0123456789abcdefghijklmnopqrstuvwxyz'

    if number == 0:
        return '0'

    result = ''
    while number > 0:
        number, remainder = divmod(number, 36)
        result = chars[remainder] + result

    return result

def generate_short_id(fallback=False):
    """
    Generate a short, sequential alphanumeric ID.

    Args:
        fallback: If True, use a fallback mechanism to generate a unique ID
                 This is useful when the sequential approach fails due to collisions

    Returns:
        A 2+ character alphanumeric ID (base-36 encoding)
    """
    # If fallback is requested, generate a random short ID
    if fallback:
        # Generate a random 4-character base-36 ID (more than enough to avoid collisions)
        import random
        import string
        return ''.join(random.choice(ID_CHARSET) for _ in range(FALLBACK_ID_LENGTH))

    with get_connection() as conn:
        # Use a transaction to ensure atomicity
        conn.execute("BEGIN EXCLUSIVE TRANSACTION")
        try:
            c = conn.cursor()
            # Get all short IDs that are purely numeric or alphanumeric with length 2-3
            # This filters out the random IDs generated by the fallback mechanism
            c.execute(f"SELECT short_id FROM nodes WHERE length(short_id) <= {SHORT_ID_MAX_LENGTH} AND short_id GLOB '[0-9a-z]*'")
            short_ids = [row[0] for row in c.fetchall()]

            if not short_ids:
                # First node gets '01'
                return '01'

            # Find the maximum valid base-36 value
            max_value = 0
            for short_id in short_ids:
                try:
                    # Convert to integer using base-36 (0-9, a-z)
                    value = int(short_id, 36)
                    max_value = max(max_value, value)
                except ValueError:
                    # Skip IDs that can't be parsed as base-36
                    continue

            # Increment by 1 in base-36
            next_value = max_value + 1

            # Convert to base-36 string (using base-36 instead of hex)
            # This ensures IDs increase by 1 in their displayed form
            new_id = base36_encode(next_value)

            # Ensure at least minimum characters
            if len(new_id) < MIN_SHORT_ID_LENGTH:
                new_id = new_id.zfill(MIN_SHORT_ID_LENGTH)

            return new_id
        finally:
            # Ensure the transaction is committed even if an error occurs
            conn.commit()

def initialize_db(erase=False, create_root_node=True, migrate=True):
    """
    Initialize the database.

    Args:
        erase (bool): If True and the database exists, it will be erased.
                     If False and the database exists, it will not be modified.
        create_root_node (bool): If True, creates a default root node if no nodes exist.
        migrate (bool): If True, run migrations to update the database schema.
    """
    db_path = get_db_path()
    if erase and os.path.exists(db_path):
        os.remove(db_path)

    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            CREATE TABLE IF NOT EXISTS nodes (
                id TEXT PRIMARY KEY,
                short_id TEXT UNIQUE,
                content TEXT NOT NULL,
                parent_id TEXT,
                role TEXT,
                provider TEXT,
                model TEXT,
                FOREIGN KEY(parent_id) REFERENCES nodes(id)
            )
        """)
        c.execute("""
            CREATE TABLE IF NOT EXISTS meta (
                key TEXT PRIMARY KEY,
                value TEXT NOT NULL
            )
        """)
        c.execute("""
            CREATE TABLE IF NOT EXISTS topics (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                start_node_id TEXT NOT NULL,
                end_node_id TEXT,
                confidence TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(start_node_id) REFERENCES nodes(id),
                FOREIGN KEY(end_node_id) REFERENCES nodes(id)
            )
        """)
        c.execute("""
            CREATE TABLE IF NOT EXISTS compressions (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                compressed_node_id TEXT NOT NULL,
                original_branch_head TEXT NOT NULL,
                original_node_count INTEGER NOT NULL,
                original_words INTEGER NOT NULL,
                compressed_words INTEGER NOT NULL,
                compression_ratio REAL NOT NULL,
                strategy TEXT NOT NULL,
                duration_seconds REAL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(compressed_node_id) REFERENCES nodes(id),
                FOREIGN KEY(original_branch_head) REFERENCES nodes(id)
            )
        """)
        c.execute("""
            CREATE TABLE IF NOT EXISTS topic_detection_scores (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                user_node_id TEXT NOT NULL,
                detection_timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                
                -- Core detection result
                topic_changed BOOLEAN NOT NULL,
                detection_method TEXT,  -- 'hybrid', 'llm', 'llm_fallback', 'too_few_messages', etc.
                final_score REAL,
                
                -- Hybrid detection scores (if used)
                semantic_drift_score REAL,
                keyword_explicit_score REAL,
                keyword_domain_score REAL,
                message_gap_score REAL,
                conversation_flow_score REAL,
                
                -- Additional metadata
                transition_phrase TEXT,  -- Detected transition phrase if any
                detected_domains TEXT,   -- JSON of detected domain scores
                dominant_domain TEXT,    -- Current dominant domain
                previous_domain TEXT,    -- Previous dominant domain
                
                -- LLM detection info (if used)
                llm_response TEXT,       -- Raw LLM response
                llm_confidence REAL,     -- LLM confidence score if available
                
                -- Context info
                user_messages_in_topic INTEGER,
                total_topics_count INTEGER,
                effective_threshold INTEGER,
                
                -- Boundary analysis (if performed)
                boundary_analyzed BOOLEAN DEFAULT FALSE,
                actual_transition_node_id TEXT,  -- Where topic actually changed
                
                FOREIGN KEY(user_node_id) REFERENCES nodes(id),
                FOREIGN KEY(actual_transition_node_id) REFERENCES nodes(id)
            )
        """)
        conn.commit()

        # Run migrations if requested
        if migrate:
            # Migrate short IDs
            migrate_to_short_ids()
            # Migrate roles
            migrate_to_roles()
            # Migrate provider and model
            migrate_to_provider_model()
            # Ensure compression tables exist (includes content column migration)
            from episodic.db_compression import create_compression_tables
            create_compression_tables()
            # Make end_node_id nullable in topics table
            migrate_topics_nullable_end()

        # Check if we should create a root node and if there are no existing nodes
        if create_root_node:
            c.execute("SELECT COUNT(*) FROM nodes")
            node_count = c.fetchone()[0]

            if node_count == 0:
                # Close the current connection to allow insert_node to create its own
                conn.commit()

                # Create a default root node with an empty string and system role
                root_node_id, root_short_id = insert_node("", None, role="system")

                # Return the root node ID
                return root_node_id, root_short_id

    return None

def insert_node(content, parent_id=None, role=None, provider=None, model=None, max_retries=MAX_DATABASE_RETRIES):
    """
    Insert a new node into the database.

    Args:
        content: The content of the node
        parent_id: The ID of the parent node (optional)
        role: The role of the node (e.g., "user", "assistant") (optional)
        provider: The provider that generated this node (e.g., "openai", "anthropic") (optional)
        model: The specific model that generated this node (e.g., "gpt-4", "claude-3") (optional)
        max_retries: Maximum number of retries if a duplicate short_id is generated

    Returns:
        Tuple of (node_id, short_id)

    Raises:
        sqlite3.IntegrityError: If a unique constraint is violated after max_retries
    """
    node_id = str(uuid.uuid4())

    # Try to insert with retries for duplicate short_id
    retries = 0
    while retries <= max_retries:
        try:
            # On first attempt, use sequential ID. On retries, use fallback random ID
            use_fallback = retries > 0
            short_id = generate_short_id(fallback=use_fallback)

            with get_connection() as conn:
                c = conn.cursor()
                c.execute(
                    "INSERT INTO nodes (id, short_id, content, parent_id, role, provider, model) VALUES (?, ?, ?, ?, ?, ?, ?)",
                    (node_id, short_id, content, parent_id, role, provider, model)
                )
                conn.commit()
            set_head(node_id)
            return node_id, short_id
        except sqlite3.IntegrityError as e:
            # If it's a duplicate short_id, retry with a new short_id
            if "UNIQUE constraint failed: nodes.short_id" in str(e) and retries < max_retries:
                retries += 1
                # Log the collision for debugging
                logger.warning(f"Short ID collision detected. Retrying ({retries}/{max_retries})...")
                continue
            # If it's another integrity error or we've exceeded max_retries, raise the exception
            raise

    # This should never be reached due to the raise in the except block
    raise sqlite3.IntegrityError("Failed to insert node after maximum retries")

def get_node(node_id):
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("SELECT id, short_id, content, parent_id, role, provider, model FROM nodes WHERE id = ?", (node_id,))
        row = c.fetchone()
        if not row:
            # Try to find by short_id
            c.execute("SELECT id, short_id, content, parent_id, role, provider, model FROM nodes WHERE short_id = ?", (node_id,))
            row = c.fetchone()
    if row:
        return {"id": row[0], "short_id": row[1], "content": row[2], "parent_id": row[3], "role": row[4], "provider": row[5], "model": row[6]}
    return None

def get_ancestry(node_id):
    ancestry = []
    current_id = node_id
    while current_id:
        node = get_node(current_id)
        if node:
            ancestry.append(node)
            current_id = node["parent_id"]
        else:
            break
    return ancestry[::-1]  # from root to current

def set_head(node_id):
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("INSERT INTO meta (key, value) VALUES ('head', ?) ON CONFLICT(key) DO UPDATE SET value=excluded.value", (node_id,))
        conn.commit()

def get_head():
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("SELECT value FROM meta WHERE key = 'head'")
        row = c.fetchone()
    if row:
        return row[0]
    return None

def migrate_to_short_ids():
    """
    Add short IDs to existing nodes.

    This function should be called after upgrading to a version that supports short IDs.
    It will add the short_id column to the nodes table if it doesn't exist,
    and generate short IDs for all existing nodes that don't have one.

    Returns:
        The number of nodes that were updated with short IDs
    """
    with get_connection() as conn:
        c = conn.cursor()

        # Check if short_id column exists
        c.execute("PRAGMA table_info(nodes)")
        columns = [info[1] for info in c.fetchall()]

        if 'short_id' not in columns:
            # Add the column
            c.execute("ALTER TABLE nodes ADD COLUMN short_id TEXT")
            c.execute("CREATE UNIQUE INDEX IF NOT EXISTS idx_short_id ON nodes(short_id)")

        # Get all nodes without short IDs
        c.execute("SELECT id FROM nodes WHERE short_id IS NULL ORDER BY ROWID")
        nodes = c.fetchall()

        # Assign sequential short IDs
        count = 0

        # Get the highest existing short ID
        c.execute("SELECT MAX(short_id) FROM nodes WHERE short_id IS NOT NULL")
        result = c.fetchone()[0]

        # Start with '01' if no short IDs exist
        next_value = 1

        # If there are existing short IDs, start from the next value
        if result:
            try:
                next_value = int(result, 36) + 1
            except ValueError:
                # If there's an error parsing the existing short_id, start with '01'
                next_value = 1

        # Assign sequential short IDs
        for node_id, in nodes:
            # Convert to base-36 string (using base-36 instead of hex)
            # This ensures IDs increase by 1 in their displayed form
            short_id = base36_encode(next_value)

            # Ensure at least 2 characters
            if len(short_id) < 2:
                short_id = short_id.zfill(2)

            next_value += 1

            c.execute("UPDATE nodes SET short_id = ? WHERE id = ?", (short_id, node_id))
            count += 1

        conn.commit()
        return count

def get_recent_nodes(limit=5):
    """
    Get the most recent nodes added to the database.

    Args:
        limit (int): Maximum number of nodes to retrieve

    Returns:
        List of node dictionaries, ordered by recency (most recent first)
    """
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            SELECT id, short_id, content, parent_id, role, provider, model
            FROM nodes
            ORDER BY ROWID DESC
            LIMIT ?
        """, (limit,))

        # Get column names from cursor description
        columns = [desc[0] for desc in c.description]
        rows = c.fetchall()

    # Create a list of dictionaries with column names as keys
    result = []
    for row in rows:
        node = {}
        for i, column in enumerate(columns):
            node[column] = row[i]
        result.append(node)

    return result

def get_all_nodes():
    """
    Retrieve all nodes from the database.

    Returns:
        List of dictionaries containing node data (id, short_id, content, parent_id, role)
    """
    try:
        with get_connection() as conn:
            c = conn.cursor()
            c.execute("SELECT id, short_id, content, parent_id, role, provider, model FROM nodes")

            # Get column names from cursor description
            columns = [desc[0] for desc in c.description]
            rows = c.fetchall()

        # Create a list of dictionaries with column names as keys
        result = []
        for row in rows:
            node = {}
            for i, column in enumerate(columns):
                node[column] = row[i]
            result.append(node)
        return result
    except Exception as e:
        logger.error(f"Error retrieving nodes from database: {str(e)}")
        return []

def get_descendants(node_id):
    """
    Get all descendants of a node.

    Args:
        node_id: ID of the node

    Returns:
        List of IDs of all descendants
    """
    descendants = []

    with get_connection() as conn:
        c = conn.cursor()

        # Use a recursive CTE to find all descendants
        c.execute("""
            WITH RECURSIVE descendants(id) AS (
                SELECT id FROM nodes WHERE parent_id = ?
                UNION ALL
                SELECT n.id FROM nodes n, descendants d WHERE n.parent_id = d.id
            )
            SELECT id FROM descendants
        """, (node_id,))

        descendants = [row[0] for row in c.fetchall()]

    return descendants

def get_children(node_id):
    """
    Get immediate children of a node.
    
    Args:
        node_id: ID of the parent node
        
    Returns:
        List of child node dictionaries
    """
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            SELECT id, short_id, content, parent_id, role, provider, model
            FROM nodes
            WHERE parent_id = ?
            ORDER BY ROWID
        """, (node_id,))
        
        columns = ['id', 'short_id', 'content', 'parent_id', 'role', 'provider', 'model']
        children = []
        for row in c.fetchall():
            child = {}
            for i, column in enumerate(columns):
                child[column] = row[i]
            children.append(child)
        
        return children

def delete_node(node_id):
    """
    Delete a node and all its descendants from the database.

    Args:
        node_id: ID of the node to delete

    Returns:
        List of IDs of all deleted nodes
    """
    # Resolve the node reference if it's not a UUID
    node_id = resolve_node_ref(node_id)

    # Check if the node exists
    node = get_node(node_id)
    if not node:
        return []

    # Get all descendants
    descendants = get_descendants(node_id)

    # Add the node itself to the list of nodes to delete
    nodes_to_delete = [node_id] + descendants

    # Delete all nodes
    with get_connection() as conn:
        c = conn.cursor()

        # Check if the node to delete is the current head
        c.execute("SELECT value FROM meta WHERE key = 'head'")
        head_id = c.fetchone()

        if head_id and head_id[0] in nodes_to_delete:
            # If the head is being deleted, set the parent as the new head
            parent_id = node['parent_id']
            if parent_id:
                set_head(parent_id)
            else:
                # If there's no parent, find another root node
                c.execute("SELECT id FROM nodes WHERE parent_id IS NULL AND id != ? LIMIT 1", (node_id,))
                new_head = c.fetchone()
                if new_head:
                    set_head(new_head[0])
                else:
                    # If there are no other root nodes, remove the head reference
                    c.execute("DELETE FROM meta WHERE key = 'head'")

        # Delete all nodes
        placeholders = ','.join(['?'] * len(nodes_to_delete))
        c.execute(f"DELETE FROM nodes WHERE id IN ({placeholders})", nodes_to_delete)
        conn.commit()

    return nodes_to_delete

def resolve_node_ref(ref):
    """
    Resolve a node reference to its UUID.

    Args:
        ref: A node reference, which can be:
            - A UUID
            - A short ID
            - "HEAD" (case insensitive) or "@head"
            - A relative reference like "HEAD~n"

    Returns:
        The UUID of the referenced node, or None if the reference cannot be resolved
    """
    if not ref:
        return None

    # Handle special references
    if ref == "@head" or ref.upper() == "HEAD" or ref.lower() == "head":
        return get_head()

    # Handle relative references
    if ref.lower().startswith("head~"):
        try:
            steps_back = int(ref[len("head~"):])
        except ValueError:
            raise ValueError(f"Invalid relative head reference: {ref}")

        current_id = get_head()
        for _ in range(steps_back):
            node = get_node(current_id)
            if node is None or node['parent_id'] is None:
                return None
            current_id = node['parent_id']
        return current_id

    # Check if it's a short ID
    node = get_node(ref)
    if node:
        return node['id']

    # If we get here, assume it's already a UUID
    return ref

def migrate_to_provider_model():
    """
    Add provider and model information to existing nodes.

    This function should be called after upgrading to a version that supports provider and model tracking.
    It will add provider and model columns to the nodes table if they don't exist.

    Returns:
        The number of columns added (0, 1, or 2)
    """
    with get_connection() as conn:
        c = conn.cursor()

        # Check if columns exist
        c.execute("PRAGMA table_info(nodes)")
        columns = [info[1] for info in c.fetchall()]
        columns_added = 0

        # Add provider column if it doesn't exist
        if 'provider' not in columns:
            c.execute("ALTER TABLE nodes ADD COLUMN provider TEXT")
            columns_added += 1

        # Add model column if it doesn't exist
        if 'model' not in columns:
            c.execute("ALTER TABLE nodes ADD COLUMN model TEXT")
            columns_added += 1

        conn.commit()
        return columns_added


def migrate_topics_nullable_end():
    """
    Make end_node_id nullable in the topics table.
    
    SQLite doesn't support ALTER COLUMN, so we need to recreate the table.
    """
    with get_connection() as conn:
        c = conn.cursor()
        
        # Check if topics table exists
        c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='topics'")
        if not c.fetchone():
            return  # No topics table to migrate
        
        # Check if end_node_id is already nullable (by checking if there are any NULL values)
        # This is a workaround since SQLite doesn't expose NOT NULL constraints easily
        try:
            c.execute("INSERT INTO topics (name, start_node_id, end_node_id, confidence) VALUES ('test', 'test', NULL, 'test')")
            # If we get here, it's already nullable
            c.execute("DELETE FROM topics WHERE name='test' AND start_node_id='test'")
            conn.commit()
            return
        except sqlite3.IntegrityError:
            # NOT NULL constraint exists, need to migrate
            conn.rollback()
        
        # Create new table with nullable end_node_id
        c.execute("""
            CREATE TABLE topics_new (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                name TEXT NOT NULL,
                start_node_id TEXT NOT NULL,
                end_node_id TEXT,
                confidence TEXT,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                FOREIGN KEY(start_node_id) REFERENCES nodes(id),
                FOREIGN KEY(end_node_id) REFERENCES nodes(id)
            )
        """)
        
        # Copy data from old table
        c.execute("""
            INSERT INTO topics_new (id, name, start_node_id, end_node_id, confidence, created_at)
            SELECT id, name, start_node_id, end_node_id, confidence, created_at FROM topics
        """)
        
        # Drop old table and rename new one
        c.execute("DROP TABLE topics")
        c.execute("ALTER TABLE topics_new RENAME TO topics")
        
        conn.commit()

def migrate_to_roles():
    """
    Add role information to existing nodes.

    This function should be called after upgrading to a version that supports roles.
    It will add the role column to the nodes table if it doesn't exist,
    and infer roles for all existing nodes based on their position in the conversation.

    Returns:
        The number of nodes that were updated with roles
    """
    with get_connection() as conn:
        c = conn.cursor()

        # Check if role column exists
        c.execute("PRAGMA table_info(nodes)")
        columns = [info[1] for info in c.fetchall()]

        if 'role' not in columns:
            # Add the column
            c.execute("ALTER TABLE nodes ADD COLUMN role TEXT")

        # Get all nodes without roles
        c.execute("SELECT id FROM nodes WHERE role IS NULL ORDER BY ROWID")
        nodes_without_roles = [row[0] for row in c.fetchall()]

        if not nodes_without_roles:
            return 0  # No nodes to update

        # Get all nodes
        c.execute("SELECT id, parent_id FROM nodes ORDER BY ROWID")
        all_nodes = {row[0]: row[1] for row in c.fetchall()}

        # Build a tree structure to determine the role of each node
        node_roles = {}
        count = 0

        # First, identify root nodes (nodes with no parent)
        root_nodes = [node_id for node_id, parent_id in all_nodes.items() if parent_id is None]

        # For each root node, assign it the role "system"
        for node_id in root_nodes:
            node_roles[node_id] = "system"
            count += 1

        # For each node with a parent, determine its role based on its parent's role
        for node_id, parent_id in all_nodes.items():
            if parent_id is None:
                continue  # Skip root nodes, already processed

            # If the parent has a role, assign the opposite role
            if parent_id in node_roles:
                parent_role = node_roles[parent_id]
                if parent_role == "user":
                    node_roles[node_id] = "assistant"
                elif parent_role == "assistant":
                    node_roles[node_id] = "user"
                elif parent_role == "system":
                    # If the parent is a system message, the child is a user message
                    node_roles[node_id] = "user"
                count += 1

        # Update the database with the inferred roles
        for node_id, role in node_roles.items():
            c.execute("UPDATE nodes SET role = ? WHERE id = ?", (role, node_id))

        conn.commit()
        return count


def store_topic(name: str, start_node_id: str, end_node_id: Optional[str] = None, confidence: str = None):
    """
    Store a topic with its conversation range.
    
    Args:
        name: Topic name (e.g., "movies", "quantum-physics")
        start_node_id: ID of the first node in this topic
        end_node_id: Optional ID of the last node in this topic (None for ongoing topics)
        confidence: Optional confidence level (high, medium, low)
    
    Returns:
        The ID of the inserted topic record
    """
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            INSERT INTO topics (name, start_node_id, end_node_id, confidence)
            VALUES (?, ?, ?, ?)
        """, (name, start_node_id, end_node_id, confidence))
        return c.lastrowid


def get_recent_topics(limit: int = 10):
    """
    Get recent topics with their conversation ranges.
    
    Args:
        limit: Maximum number of topics to return
    
    Returns:
        List of topic dictionaries with name, node range, and metadata
    """
    with get_connection() as conn:
        c = conn.cursor()
        
        # Build query with optional LIMIT
        query = """
            SELECT t.name, t.start_node_id, t.end_node_id, t.confidence, t.created_at,
                   n1.short_id as start_short_id, n2.short_id as end_short_id
            FROM topics t
            JOIN nodes n1 ON t.start_node_id = n1.id
            LEFT JOIN nodes n2 ON t.end_node_id = n2.id
            ORDER BY n1.ROWID ASC
        """
        
        if limit is not None:
            query += " LIMIT ?"
            c.execute(query, (limit,))
        else:
            c.execute(query)
        
        topics = []
        for row in c.fetchall():
            topics.append({
                'name': row[0],
                'start_node_id': row[1],
                'end_node_id': row[2],
                'confidence': row[3],
                'created_at': row[4],
                'start_short_id': row[5],
                'end_short_id': row[6]
            })
        
        return topics


def get_all_topics():
    """
    Get all topics ordered by creation date.
    
    Returns:
        List of all topic dictionaries
    """
    return get_recent_topics(limit=1000)  # Large limit to get all


def update_topic_end_node(topic_name: str, start_node_id: str, new_end_node_id: str):
    """
    Update the end node of an existing topic.
    
    Args:
        topic_name: Name of the topic to update
        start_node_id: Start node ID (for verification)
        new_end_node_id: New end node ID to set
    """
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            UPDATE topics 
            SET end_node_id = ?
            WHERE name = ? AND start_node_id = ?
        """, (new_end_node_id, topic_name, start_node_id))


def update_topic_name(old_name: str, start_node_id: str, new_name: str):
    """
    Update the name of an existing topic.
    
    Args:
        old_name: Current name of the topic
        start_node_id: Start node ID (for verification)
        new_name: New name for the topic
    
    Returns:
        Number of rows updated
    """
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            UPDATE topics 
            SET name = ?
            WHERE name = ? AND start_node_id = ?
        """, (new_name, old_name, start_node_id))
        return c.rowcount


def store_compression(compressed_node_id: str, original_branch_head: str, 
                     original_node_count: int, original_words: int, compressed_words: int,
                     compression_ratio: float, strategy: str, duration_seconds: float = None):
    """
    Store compression metadata in the database.
    
    Args:
        compressed_node_id: ID of the newly created compressed node
        original_branch_head: ID of the last node in the original branch
        original_node_count: Number of nodes in the original branch
        original_words: Total word count in original branch
        compressed_words: Word count in compressed summary
        compression_ratio: Percentage reduction
        strategy: Compression strategy used ('simple', 'key-moments')
        duration_seconds: Time taken to compress (optional)
    
    Returns:
        The ID of the inserted compression record
    """
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            INSERT INTO compressions (
                compressed_node_id, original_branch_head, original_node_count,
                original_words, compressed_words, compression_ratio,
                strategy, duration_seconds
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        """, (compressed_node_id, original_branch_head, original_node_count,
              original_words, compressed_words, compression_ratio,
              strategy, duration_seconds))
        return c.lastrowid


def get_compression_stats():
    """
    Get compression statistics from the database.
    
    Returns:
        Dictionary with compression statistics
    """
    with get_connection() as conn:
        c = conn.cursor()
        
        # Check if compressions table exists
        c.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='compressions'")
        if not c.fetchone():
            return {
                'total_compressions': 0,
                'total_words_saved': 0,
                'average_compression_ratio': 0,
                'strategies_used': {}
            }
        
        # Get overall stats
        c.execute("""
            SELECT 
                COUNT(*) as total_compressions,
                SUM(original_words - compressed_words) as total_words_saved,
                AVG(compression_ratio) as avg_ratio
            FROM compressions
        """)
        
        row = c.fetchone()
        total_compressions = row[0] or 0
        total_words_saved = row[1] or 0
        avg_ratio = row[2] or 0
        
        # Get strategy breakdown
        c.execute("""
            SELECT strategy, COUNT(*) as count, AVG(compression_ratio) as avg_ratio
            FROM compressions
            GROUP BY strategy
        """)
        
        strategies = {}
        for row in c.fetchall():
            strategies[row[0]] = {
                'count': row[1],
                'avg_ratio': row[2]
            }
        
        return {
            'total_compressions': total_compressions,
            'total_words_saved': total_words_saved,
            'average_compression_ratio': avg_ratio,
            'strategies_used': strategies
        }


def store_topic_detection_scores(
    user_node_id: str,
    topic_changed: bool,
    detection_method: str,
    final_score: float = None,
    semantic_drift_score: float = None,
    keyword_explicit_score: float = None,
    keyword_domain_score: float = None,
    message_gap_score: float = None,
    conversation_flow_score: float = None,
    transition_phrase: str = None,
    detected_domains: str = None,
    dominant_domain: str = None,
    previous_domain: str = None,
    llm_response: str = None,
    llm_confidence: float = None,
    user_messages_in_topic: int = None,
    total_topics_count: int = None,
    effective_threshold: int = None,
    boundary_analyzed: bool = False,
    actual_transition_node_id: str = None
):
    """
    Store topic detection scores for debugging and analysis.
    
    Args:
        user_node_id: The user message node being analyzed
        topic_changed: Whether a topic change was detected
        detection_method: Method used ('hybrid', 'llm', 'too_few_messages', etc.)
        final_score: Final combined score (for hybrid detection)
        semantic_drift_score: Embedding-based drift score
        keyword_explicit_score: Explicit transition phrase score
        keyword_domain_score: Domain shift score
        message_gap_score: Time/message gap score
        conversation_flow_score: Conversation pattern score
        transition_phrase: Detected transition phrase if any
        detected_domains: JSON string of domain scores
        dominant_domain: Current dominant domain
        previous_domain: Previous dominant domain
        llm_response: Raw LLM response
        llm_confidence: LLM confidence score
        user_messages_in_topic: Number of user messages in current topic
        total_topics_count: Total number of topics
        effective_threshold: Effective threshold used
        boundary_analyzed: Whether boundary analysis was performed
        actual_transition_node_id: Node where topic actually changed
    """
    with get_connection() as conn:
        c = conn.cursor()
        c.execute("""
            INSERT INTO topic_detection_scores (
                user_node_id, topic_changed, detection_method, final_score,
                semantic_drift_score, keyword_explicit_score, keyword_domain_score,
                message_gap_score, conversation_flow_score,
                transition_phrase, detected_domains, dominant_domain, previous_domain,
                llm_response, llm_confidence,
                user_messages_in_topic, total_topics_count, effective_threshold,
                boundary_analyzed, actual_transition_node_id
            ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            user_node_id, topic_changed, detection_method, final_score,
            semantic_drift_score, keyword_explicit_score, keyword_domain_score,
            message_gap_score, conversation_flow_score,
            transition_phrase, detected_domains, dominant_domain, previous_domain,
            llm_response, llm_confidence,
            user_messages_in_topic, total_topics_count, effective_threshold,
            boundary_analyzed, actual_transition_node_id
        ))
        conn.commit()


def get_topic_detection_scores(user_node_id: str = None, limit: int = 100):
    """
    Retrieve topic detection scores for analysis.
    
    Args:
        user_node_id: Optional specific node to get scores for
        limit: Maximum number of records to return
        
    Returns:
        List of score records
    """
    with get_connection() as conn:
        c = conn.cursor()
        
        if user_node_id:
            c.execute("""
                SELECT * FROM topic_detection_scores
                WHERE user_node_id = ?
                ORDER BY detection_timestamp DESC
            """, (user_node_id,))
        else:
            c.execute("""
                SELECT * FROM topic_detection_scores
                ORDER BY detection_timestamp DESC
                LIMIT ?
            """, (limit,))
        
        # Get column names
        columns = [desc[0] for desc in c.description]
        rows = c.fetchall()
        
        # Convert to list of dicts
        results = []
        for row in rows:
            record = {}
            for i, col in enumerate(columns):
                record[col] = row[i]
            results.append(record)
        
        return results


